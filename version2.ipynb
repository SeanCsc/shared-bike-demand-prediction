{"nbformat": 4, "metadata": {"language_info": {"file_extension": ".py", "pygments_lexer": "ipython3", "version": "3.6.4", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "nbformat_minor": 1, "cells": [{"outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "%matplotlib inline\n", "from sklearn.model_selection import KFold\n", "from sklearn.pipeline import make_pipeline\n", "from sklearn.preprocessing import RobustScaler\n", "from sklearn.linear_model import LassoCV, Lasso, ElasticNet\n", "from sklearn.kernel_ridge import KernelRidge\n", "from xgboost import XGBRegressor\n", "from sklearn import cross_validation,metrics\n", "from sklearn.grid_search import GridSearchCV\n", "from sklearn.ensemble import GradientBoostingRegressor\n", "from lightgbm import LGBMRegressor\n", "from pandas.tseries.holiday import USFederalHolidayCalendar\n", "from pandas.tseries.offsets import CustomBusinessDay\n", "from sklearn.model_selection import train_test_split\n", "import math\n", "import matplotlib.pyplot as plt\n", "from sklearn.feature_selection import SelectKBest\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.decomposition import PCA\n", "from sklearn.grid_search import GridSearchCV\n", "from sklearn.pipeline import FeatureUnion\n", "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n", "from sklearn.tree import DecisionTreeRegressor\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.cross_validation import KFold\n", "from sklearn.metrics import mean_squared_error, median_absolute_error\n", "import xgboost as xgb\n", "import datetime as datetime\n", "import pandas as pd\n", "import numpy as np\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "# Load in our libraries\n", "import pandas as pd\n", "import numpy as np\n", "import re\n", "import sklearn\n", "import xgboost as xgb\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "\n", "import plotly.offline as py\n", "py.init_notebook_mode(connected=True)\n", "import plotly.graph_objs as go\n", "import plotly.tools as tls\n", "\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "\n", "# Going to use these 5 base models for the stacking\n", "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n", "                              GradientBoostingClassifier, ExtraTreesClassifier)\n", "from sklearn.svm import SVC\n", "from sklearn.cross_validation import KFold\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "metadata": {"_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5", "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["station = pd.read_csv('../input/station.csv')\n", "station.sort_values(by = ['city'])\n", "trip = pd.read_csv('../input/trip.csv', parse_dates=['start_date'])\n", "station_city = station[['id','city']]\n"], "metadata": {"_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a", "collapsed": true, "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["def get_city_data(tripc,city):\n", "    trip = tripc[tripc['city']==city]\n", "    return trip\n", "## data merge\n", "tripc = trip.merge(station_city,left_on = 'start_station_id',right_on = 'id',how = 'inner')\n", "#extract the trip information for the city San Jose\n", "city = 'San Jose'\n", "trip_SJ = get_city_data(tripc,city)\n", "\n"], "metadata": {"_uuid": "6d0a5b10d4432ae3ee4bfc60e030ded3b3244f82", "collapsed": true, "_cell_guid": "1ce95d41-4e93-4dbe-a042-74b2c35e509c"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": [], "metadata": {"_uuid": "79f75457a4ec5432870691d43a9d82d1e855a816", "collapsed": true, "_cell_guid": "5a2d0af3-7a79-4f43-a0b6-e8645c36f1fe"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["def rmse(y_hat,y):\n", "    diff = y_hat - y\n", "    nrerr = np.mean(diff*diff)\n", "    err = np.sqrt(nrerr)\n", "    return err\n", "def get_label(trip):\n", "#Baseline model: use the mean value as the prediction result and calculate the rmse\n", "    trip['date'] = trip.start_date.dt.date\n", "#count how many trips occured on a day\n", "    dates = {}\n", "    for d in trip.date:\n", "        if d not in dates:\n", "            dates[d] = 1\n", "        if d in dates:\n", "            dates[d] = dates[d] + 1\n", "#extract the label (daily trip number and corresponding day)\n", "    trip2 = pd.DataFrame.from_dict(dates,orient = \"index\")\n", "    trip2['date'] = trip2.index\n", "    trip2['trips'] = trip2.iloc[:,0]\n", "    train = trip2.iloc[:,1:3]\n", "    train.reset_index(drop = True, inplace = True)\n", "    return train\n", "def base_model(trip):\n", "    #split the data into 80% and 20%, use the mean of 80% data and get the RMSE of 20% test data\n", "    labels = trip.loc[:,'trips']\n", "    train = trip.drop(['trips'], 1)\n", "    X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.2, random_state = 2)\n", "    y_hat = y_test\n", "    y_hat = np.mean(y_train)\n", "    res = rmse(y_hat,y_test)\n", "    return res\n", "train_SJ_label = get_label(trip_SJ)\n", "base_rmse = base_model(train_SJ_label)\n"], "metadata": {"_uuid": "4a94c4620a3d7ebb4953198d1bcbe14d61a80215", "collapsed": true, "_cell_guid": "337ac841-4e09-4052-a06e-9f8cd4e6594c", "scrolled": true}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["#Feature Engineering - add weather feature\n", "weather = pd.read_csv('../input/weather.csv')\n", "weather.date = pd.to_datetime(weather.date, format='%m/%d/%Y')\n", "weather.loc[weather.events == 'rain', 'events'] = \"Rain\"\n", "weather.loc[weather.events.isnull(), 'events'] = \"Normal\"\n", "weather = pd.get_dummies(weather,columns = [\"events\"])\n"], "metadata": {"_uuid": "1f7d911873177b2e98c4ea160d7deb826fe142f0", "collapsed": true, "_cell_guid": "23798fcb-11c3-44f4-972f-e699438674e2"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["def get_weather(weather,city):\n", "    my_dict = {'San Francisco':94107,'Redwood City':94063,'Palo Alto':94301, 'Mountain View':94041, 'San Jose':95113}\n", "    weather_c = weather[weather['zip_code'] == my_dict[city]]\n", "    return weather_c\n", "weather_SJ = get_weather(weather,city)\n", "train_SJ = train_SJ_label.merge(weather_SJ, on = weather_SJ.date)"], "metadata": {"_uuid": "931679ed8ae6d21b226f115fe2a570a06f36a4a0", "collapsed": true, "_cell_guid": "55d30d9c-05e0-4f49-a9bd-446613b29cef"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["#fill in the missing data, dtype transform\n", "def preprocess(train_SJ):\n", "    train_SJ['date'] = train_SJ['date_x']\n", "    train_SJ.drop(['date_y','date_x','zip_code'],1, inplace= True)\n", "    train_SJ.precipitation_inches = pd.to_numeric(train_SJ.precipitation_inches,errors = 'coerce')\n", "    train_SJ.loc[train_SJ.max_gust_speed_mph.isnull(),'max_gust_speed_mph'] = train_SJ.groupby('max_wind_Speed_mph').max_gust_speed_mph.apply(lambda x: x.fillna(x.median()))\n", "    train_SJ.loc[train_SJ.precipitation_inches.isnull(), \n", "            'precipitation_inches'] = train_SJ[train_SJ.precipitation_inches.notnull()].precipitation_inches.median()    #transform the dtype to numeric\n", "    return train_SJ"], "metadata": {"_uuid": "fab8c8e4a258966bf4822a772f31cf2ab7e38af0", "collapsed": true, "_cell_guid": "3fa11f3a-59eb-4d90-887e-8d5bf17f33c8"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": [], "metadata": {"_uuid": "dbe5434cd12916f4a9b58818c312a796467b4dcf", "collapsed": true, "_cell_guid": "7899d81d-3de7-4618-8af0-f9708b96b7f6"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["train_SJ = preprocess(train_SJ) "], "metadata": {"_uuid": "0b821b1a7fe3f654150af5577a8ba50f530df64e", "collapsed": true, "_cell_guid": "5291686e-f551-4cc1-821e-21d590e6d1ad"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["####add station\n", "def get_docks(train,station,city):\n", "    stationc = station[station['city'] == city]\n", "    stationc.installation_date = pd.to_datetime(stationc.installation_date, format = \"%m/%d/%Y\").dt.date\n", "    total_docks = []\n", "    for day in train.date:\n", "        total_docks.append(sum(stationc[stationc.installation_date <= day].dock_count))\n", "    return total_docks\n"], "metadata": {"_uuid": "ab48c17dfb9c93855a2d4ac78a83aaa2331385ad", "collapsed": true, "_cell_guid": "4998038a-23ab-4b1c-904c-1e6ef8765650", "scrolled": false}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["dock_SJ = get_docks(train_SJ,station,city)\n", "train_SJ['dock'] = dock_SJ"], "metadata": {"_uuid": "fc4814a85640c2ec0502659296a9c82a8c4b77c8", "collapsed": true, "_cell_guid": "9048685b-e701-4960-a31c-3236c216be90"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["#Add datetime feature\n", "def add_DateFeature(train):\n", "    calendar = USFederalHolidayCalendar()\n", "    holidays = calendar.holidays(start=train_SJ.date.min(), end=train_SJ.date.max())\n", "    us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n", "    business_days = pd.DatetimeIndex(start=train_SJ.date.min(), end=train_SJ.date.max(), freq=us_bd)\n", "    businessday = pd.to_datetime(business_days, format='%Y/%m/%d').date\n", "    holiday = pd.to_datetime(holidays, format='%Y/%m/%d').date\n", "    train['holiday'] = train.date.isin(holidays)\n", "    train['businessday'] = train.date.isin(businessday)\n", "    train.holiday = train.holiday.map(lambda x : 1 if x == True else 0)\n", "    train.businessday = train.businessday.map(lambda x : 1 if x == True else 0)\n", "    train['year'] = pd.to_datetime(train['date']).dt.year\n", "    train['month'] = pd.to_datetime(train['date']).dt.month\n", "    train['weekday'] = pd.to_datetime(train['date']).dt.weekday\n", "    return train\n"], "metadata": {"_uuid": "c73f9ad77e3f3bef95d059f8585f917f7c3b79b8", "collapsed": true, "_cell_guid": "2a038340-045e-400c-b9cf-5bc863d82d52"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["train_SJ = add_DateFeature(train_SJ)"], "metadata": {"_uuid": "2ca4646b550253745207c943689908c2bb4c80ee", "collapsed": true, "_cell_guid": "99454f8d-43ae-480e-9624-38412ff85b1a"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["y = train_SJ['trips']\n", "date = train_SJ['date']\n", "train_SJ = train_SJ.drop(['trips','date'],1)\n"], "metadata": {"_uuid": "f5876f8e0587a34edcc031a736288f3d95311174", "collapsed": true, "_cell_guid": "b45bc7bd-dccf-45df-bf99-bbdaa82de399"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["train_SJ.describe()"], "metadata": {"_uuid": "42c664106108d2cf1ea52a985a826cca03269369", "_cell_guid": "5816db75-3aa0-4e52-9f0e-7aadca9b5781"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["x_train, x_test, y_train, y_test = train_test_split(train_SJ, y, test_size=0.1, random_state = 2)\n", "Lasso = make_pipeline(RobustScaler(),LassoCV(alphas = [1, 0.1, 0.001, 0.0005], random_state = 1))\n", "Lasso.fit(x_train,y_train)\n", "print(x_test.shape)\n", "print(x_train.shape)"], "metadata": {"_uuid": "2ee2644eed996189d04734b8f2c035200e352eb6", "_cell_guid": "cc2fee43-52a3-42c5-a2b3-95f8503e3fa6"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": [], "metadata": {"_uuid": "08ab91e336a109b73a92299d760168ae31a266fe", "collapsed": true, "_cell_guid": "45689165-144f-4232-bab8-b8c6dc2665fd"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["#linear model\n", "n_folds = 5\n", "\n", "def linear_model(train,labels):\n", "    x_train, x_test, y_train, y_test = train_test_split(train, labels, test_size=0.2, random_state = 2)\n", "    Lasso = make_pipeline(RobustScaler(),LassoCV(alphas = [1, 0.1, 0.001, 0.0005], random_state = 1))\n", "    Lasso.fit(x_train,y_train)\n", "    Lasso_predict = Lasso.predict(x_test)\n", "    rmse_err = rmse(Lasso_predict,y_test)\n", "    return rmse_err\n", "linear_model(train_SJ,y)\n", "\n", "    "], "metadata": {"_uuid": "d74f97424a69c0100d9c7f8716c2fb37a2ffd596", "_cell_guid": "e0a6c84f-40c2-427c-a3a2-682e15647594"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["#XGBoost\n", "#XGBoost Model Tuning\n", "\n", "def xgb_model(train,labels):\n", "    x_train, x_val, y_val,y_val = train_test_split(train,labels, test_size=0.1, random_state = 2)\n", "    xgb = XGBRegressor(n_estimators = 1000,learning_rate = 0.3,max_depth = 3,eval_metrics = 'rmse',\n", "                       subsample=0.8,\n", "                      gamma = 0.4,\n", "                      min_child_weight = 5)\n", "    xgb.fit(x_train,y_train,early_stopping_rounds = 5,verbose = False)\n", "    xgb_prediction = xgb.predict(x_test)\n", "    err = rmse(y_test,xgb_prediction)\n", "    return err"], "metadata": {"_uuid": "ced55a29e574f89acc4401a130d5a2fed5ce2d4c", "collapsed": true, "_cell_guid": "85f89e4d-3e68-4a13-bc8b-dce75d826aab"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["#XGBoost\n", "#XGBoost Model Tuning\n", "def xgb_model(train,labels):\n", "    x_train, x_val, y_val,y_val = train_test_split(train,labels, test_size=0.1, random_state = 2)\n", "    xgb = XGBRegressor(n_estimators = 1000,learning_rate = 0.3,max_depth = 3,eval_metrics = 'rmse',\n", "                       subsample=0.8,\n", "                      gamma = 0.4,\n", "                      min_child_weight = 5)\n", "    xgb.fit(x_train,y_train,early_stopping_rounds = 5,verbose = False)\n", "    xgb_prediction = xgb.predict(x_test)\n", "    err = rmse(y_test,xgb_prediction)\n", "    return err"], "metadata": {"_uuid": "925691f9fe5186538b3f0f62f320f530297167a1", "collapsed": true, "_cell_guid": "5396fd9b-5ab3-496a-87bd-ff38473e37dd"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["random_state = 2\n", "xgb = XGBRegressor(n_estimators = 1000,learning_rate = 0.12,max_depth = 6,eval_metrics = 'rmse',\n", "                       subsample=0.8,\n", "                      gamma = 0.1,\n", "                      min_child_weight = 5,\n", "                   seed = random_state)\n"], "metadata": {"_uuid": "c37befe9f286b5614fe33a01e1fc83fbadf7ed2e", "collapsed": true, "_cell_guid": "67aa62d6-7cba-4d16-bf52-dd5e8f743c8e"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["def rmsle_cv(model,train_data,y):\n", "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n", "    rmse= np.sqrt(-cross_val_score(model, train_data, y, scoring=\"neg_mean_squared_error\", cv = kf))\n", "    return(rmse)\n", "def scoring(clf):\n", "    scores = np.sqrt(-cross_val_score(clf, x_train, y_train, cv=5, n_jobs=1, scoring = 'neg_mean_squared_error'))\n", "    score = np.mean(scores)\n", "    return scores"], "metadata": {"_uuid": "c370adb5ab6c29d7fe9ae5ead37117b783dc95b7", "collapsed": true, "_cell_guid": "94efad5f-c26e-4c4c-b0ba-a3483978110e"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["xgb.fit(x_train,y_train)\n", "np.mean(scoring(xgb))\n"], "metadata": {"_uuid": "0f5f2413c6a806f6481adb8780d9cce1497c9ae1", "_cell_guid": "5b942af0-1679-4d38-8471-cee176cf285e"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["xgb.feature_importances_"], "metadata": {"_uuid": "dbbf4e4cac516e69abad67187d1640c1c18aa148", "_cell_guid": "382b8035-b17d-44c1-a6d7-659150952063"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["gbr = GradientBoostingRegressor(learning_rate = 0.12,\n", "                                n_estimators = 150,\n", "                                max_depth = 8,\n", "                                min_samples_leaf = 1,\n", "                                random_state = 2)\n", "scoring(gbr)\n", "gbr.fit(x_train,y_train)\n"], "metadata": {"_uuid": "2989d8260f3102f5eea93d5964487c3d154e29d0", "_cell_guid": "d630fccd-aaab-40d9-b0ec-8b22635e7d85"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["#feature importance (with tree model)\n", "def plot_importance(model):\n", "    importances = model.feature_importances_\n", "    std = np.std([model.feature_importances_ for feature in model.estimators_],\n", "                 axis=0)\n", "    indices = np.argsort(importances)[::-1] \n", "    plt.figure(figsize = (8,5))\n", "    plt.title(\"Feature importances\")\n", "    plt.bar(range(x_train.shape[1]), importances[indices], color=\"r\", align=\"center\")\n", "    plt.xticks(range(x_train.shape[1]), indices)\n", "    plt.xlim([-1, x_train.shape[1]])\n", "    plt.show()\n", "i = 0\n", "for feature in x_train:\n", "    print (i, feature)\n", "    i += 1\n", "plot_importance(gbr)\n", "    "], "metadata": {"_uuid": "beea0d697ed838285a70f3f5ef89026828cfcb66", "_cell_guid": "493a5d51-745b-4962-bf8c-14b6924cc01f"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["#feature importance(with Lasso)\n", "plot_importance(gbr)"], "metadata": {"_uuid": "3bacdfd951fd5e1c2a3962e0805e7f7b829a4927", "_cell_guid": "66a7778d-f34f-4894-8c47-282760faab9b"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["#feature selection\n", "from sklearn.feature_selection import VarianceThreshold\n", "from sklearn.svm import LinearSVC\n", "from sklearn.linear_model import Lasso\n", "from sklearn.feature_selection import SelectFromModel\n", "def var_feature_sel(X):\n", "    sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n", "    sel.fit_transform(X)\n", "    return X\n", "\n", "def mod_feature_sel(X,y):\n", "    lasso = Lasso(alpha = 0.01).fit(X, y)\n", "    print (lasso.feature_importance_)\n", "    model = SelectFromModel(lasso, prefit=True)\n", "    X_new = model.transform(X)\n", "    return X_new"], "metadata": {"_uuid": "42e1cb4c0359a9156a3a99cbaebb60b6a54546ca", "collapsed": true, "_cell_guid": "aa70cb12-b99b-4a1e-969c-dab9e7fae180"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["#data \n", "ntrain = x_train.shape[0]\n", "ntest = x_test.shape[0]\n", "SEED = 0 # for reproducibility\n", "NFOLDS = 5 # set folds for out-of-fold prediction\n", "kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)"], "metadata": {"_uuid": "719b06d710426d3ed797245a8d7d469590f54509", "collapsed": true, "_cell_guid": "bc095d6f-a760-4d6d-974d-a0a9134e3eb5"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["##helper function\n", "class SklearnHelper(object):\n", "    def __init__(self, clf, seed = 0, params = None):\n", "        params['random_state'] = seed\n", "        self.clf = clf(**params)\n", "    def train(self, x_train,y_train):\n", "        self.clf.fit(x_train,y_train)\n", "    def predict(self,x):\n", "        return self.clf.predict(x)\n", "    def fit(self, x, y):\n", "        return self.clf.fit(x,y)\n", "    def feature_importances(self,x,y):\n", "        print(self.clf.fit(x,y).feature_importances_)\n", "        \n"], "metadata": {"_uuid": "762863722f28501370bdc953d1f34e7e458ce778", "collapsed": true, "_cell_guid": "8e5d7803-32d0-467c-b936-efc7786562ec"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["#when predicting the test datasets, we need to get the results predicted by first layer. But if we use the test sets, then it might\n", "#cause overfitting when fitting new model of second layer.\n", "def get_oof(clf, x_train, y_train, x_test):\n", "    oof_train = np.zeros((ntrain,))\n", "    oof_test = np.zeros((ntest,))\n", "    oof_test_skf = np.empty((NFOLDS, ntest))\n", "\n", "    for i, (train_index, test_index) in enumerate(kf):\n", "        x_tr = x_train[train_index]\n", "        y_tr = y_train[train_index]\n", "        x_te = x_train[test_index]\n", "\n", "        clf.train(x_tr, y_tr)\n", "\n", "        oof_train[test_index] = clf.predict(x_te)\n", "        oof_test_skf[i, :] = clf.predict(x_test)\n", "\n", "    oof_test[:] = oof_test_skf.mean(axis=0)\n", "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"], "metadata": {"_uuid": "70b0f84cef8cab84c55251cb4b6cd36afe9276fd", "collapsed": true, "_cell_guid": "88c5be2d-3d99-4e2a-86a2-341b07547157"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["#def parameters\n", "rf_params = {\n", "    'n_jobs': -1,\n", "    'n_estimators': 500,\n", "     'warm_start': True, \n", "     #'max_features': 0.2,\n", "    'max_depth': 6,\n", "    'min_samples_leaf': 2,\n", "    'max_features' : 'sqrt',\n", "    'verbose': 0\n", "}\n", "# AdaBoost parameters\n", "ada_params = {\n", "    'n_estimators': 500,\n", "    'learning_rate' : 0.75\n", "}\n", "gb_params = {\n", "    'n_estimators': 500,\n", "     #'max_features': 0.2,\n", "    'max_depth': 5,\n", "    'min_samples_leaf': 2,\n", "    'verbose': 0\n", "}\n"], "metadata": {"_uuid": "71b28806bd08d6277fcad25ed5317bf70259678b", "collapsed": true, "_cell_guid": "c9396f82-f124-4320-b794-2769620b8ea4"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n", "ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n", "gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n", "x_trainv = x_train.values #values \u5c31\u662f\u8bf4\u4ecedataframe\u8f6c\u5316\u6210matrix\n", "y_trainv = y_train.ravel()\n", "x_testv = x_test.values\n", "\n"], "metadata": {"_uuid": "dbb04a983525d74b4a78921ddd218edb6fabcc95", "collapsed": true, "_cell_guid": "38f3c8ad-0585-4144-8f63-4b0be6b25fee"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["#rf_oof_train, rf_oof_test = get_oof(rf,x_trainv, y_trainv, x_testv) # Random Forest\n", "ada_oof_train, ada_oof_test = get_oof(ada, x_trainv, y_trainv, x_testv) # AdaBoost \n", "gb_oof_train, gb_oof_test = get_oof(gb,x_trainv, y_trainv, x_testv) # Gradient Boost"], "metadata": {"_uuid": "7804e8c2c54606430d5a9e650385a2aa55297170", "collapsed": true, "_cell_guid": "86aa889e-bf74-435c-861d-1b2180ad3699"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["ada_feature = ada.feature_importances(x_train, y_train)\n", "gb_feature = gb.feature_importances(x_train,y_train)"], "metadata": {"_uuid": "7dc589749a81a7281a300d7a16f10fd7fea22c0d", "_cell_guid": "74dfdca4-f184-4f91-9e8a-439cc660bbd7"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["#create a dataframe for the output\n", "cols = x_train.columns.values\n", "# Create a dataframe with features\n"], "metadata": {"_uuid": "929c97581fc1337f0525d29784e618e8fa6e657d", "collapsed": true, "_cell_guid": "7f4fd59f-cb23-4d13-9ed4-06f4bccdf749"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["#second level\n", "base_predictions_train = pd.DataFrame( {\n", "     'AdaBoost': ada_oof_train.ravel(),\n", "      'GradientBoost': gb_oof_train.ravel()\n", "    })\n", "base_predictions_train.head()"], "metadata": {"_uuid": "89b37ef53d35abfbd05a2a671ad77ed0e76e241a", "_cell_guid": "11151d3c-d4f7-41b3-95ae-3ee802d8f9da"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["data = [\n", "    go.Heatmap(\n", "        z= base_predictions_train.astype(float).corr().values ,\n", "        x=base_predictions_train.columns.values,\n", "        y= base_predictions_train.columns.values,\n", "          colorscale='Viridis',\n", "            showscale=True,\n", "            reversescale = True\n", "    )\n", "]\n", "py.iplot(data, filename='labelled-heatmap')"], "metadata": {"_uuid": "060380435766494954dd16bf66d65465e7f01d26", "collapsed": true, "_cell_guid": "05ec8982-d039-4bca-848b-09dfa30b3308"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["x_train = np.concatenate(( ada_oof_train, gb_oof_train), axis=1)\n", "x_test = np.concatenate((ada_oof_test, gb_oof_test), axis=1)"], "metadata": {"_uuid": "a73dbf0470a878517f1b3bb3e69a2524b2d1f635", "collapsed": true, "_cell_guid": "b2eda822-6234-41e3-9d73-a87c7a0a0f98"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": [" gbm = XGBRegressor(n_estimators = 2000,learning_rate = 0.01,max_depth = 6,eval_metrics = 'rmse',\n", "                       subsample=0.9,\n", "                      gamma = 0.1,\n", "                      min_child_weight = 6,\n", "                   seed = random_state).fit(x_train, y_train)\n", "predictions = gbm.predict(x_test)\n", "rmse(predictions,y_test)"], "metadata": {"_uuid": "f7c22e8da687876ca93de986d4df5c150146078a", "collapsed": true, "_cell_guid": "50ef0a72-669f-4639-b54e-e57cc05fcc04"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": [], "metadata": {"_uuid": "1f79cf4146591db2b167160a42bf0ec0cd343d8e", "collapsed": true, "_cell_guid": "19d37a26-9a45-40e9-a128-a5280d285415"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": [], "metadata": {"_uuid": "dfa52ebd679b8c05d294ded04d22230475db0226", "collapsed": true, "_cell_guid": "93307538-67c5-4718-8bf4-1d004e7f8eef"}, "execution_count": null, "cell_type": "code"}]}